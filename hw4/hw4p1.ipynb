{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSLkT0qL3jgl"
      },
      "source": [
        "# HW4P1: Language Modelling\n",
        "\n",
        "Welcome to the final part 1 hw of this course. This is the only part 1 in which you have PyTorch training (Yay). You will be working on training language models and evaluating them on the task of prediction and generation.<br>\n",
        "The model which you will be coding in this HW very similar to the Speller module from HW4P2. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "95e48c7693e34a389da49dcb6e448e0c",
        "deepnote_cell_type": "markdown",
        "id": "EB2bOV3bzYLR"
      },
      "source": [
        "# Get modules and datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "r4_-qG9rSULt"
      },
      "outputs": [],
      "source": [
        "# !pip install torchsummaryX\n",
        "# !pip install -q cohere tiktoken openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RnrUvEIC5i5j"
      },
      "outputs": [],
      "source": [
        "# TODO: Import drive if you are using Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cell_id": "03bf3bd639a048f098d5febc42e2baff",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 4,
        "execution_start": 1679856365820,
        "id": "QZNwme4320LW",
        "source_hash": "b7876178"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/astrick/Repositories/cmu-11785/h4.1\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "path = \"..\" # TODO: Add path to handout. For example D:/IDL/hw4/hw4p1_handout/handout\n",
        "sys.path.append(path) \n",
        "%cd {path}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "65e59cd2e6514d9594258167a6a0f6db",
        "deepnote_cell_type": "markdown",
        "id": "INh9p3v3zbF_"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cell_id": "b48a9e95f26c4d2e89d95b1b311cedd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution": {
          "iopub.execute_input": "2022-08-10T14:02:09.992480Z",
          "iopub.status.busy": "2022-08-10T14:02:09.987693Z",
          "iopub.status.idle": "2022-08-10T14:02:12.872562Z",
          "shell.execute_reply": "2022-08-10T14:02:12.870819Z",
          "shell.execute_reply.started": "2022-08-10T14:02:09.991351Z"
        },
        "execution_millis": 2669,
        "execution_start": 1679856365830,
        "id": "oxiZ42B4SwQ-",
        "outputId": "9888f04a-bc26-4830-d89d-5f0ab94fa875",
        "source_hash": "ec149d26",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "\n",
        "import os\n",
        "\n",
        "import time \n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Importing necessary modules from hw4\n",
        "from tests_hw4 import get_prediction_nll, make_generation_text\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device: \", DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "a4ff875589ee46da8f749a7e5088a3ef",
        "deepnote_cell_type": "markdown",
        "id": "u-R794-0zc9V"
      },
      "source": [
        "# Load datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HU4e_6l0Whda",
        "outputId": "bd0d665c-4ee9-4856-aeba-1d74df960d89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab length:  33280\n",
            "['!' '\"' '#' ... '～' '<sos>' '<eos>']\n"
          ]
        }
      ],
      "source": [
        "# Loading the vocabulary. Try printing and see\n",
        "VOCAB       = np.load('dataset/vocab.npy') \n",
        "\n",
        "# We have also included <sos> and <eos> in the vocabulary for you\n",
        "# However in real life, you include it explicitly if not provided\n",
        "SOS_TOKEN   = np.where(VOCAB == '<sos>')[0][0]\n",
        "EOS_TOKEN   = np.where(VOCAB == '<eos>')[0][0]\n",
        "NUM_WORDS   = len(VOCAB) - 2 # Actual number of words in vocabulary\n",
        "\n",
        "print(\"Vocab length: \", len(VOCAB))\n",
        "print(VOCAB)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LA7SapmyXHr7"
      },
      "outputs": [],
      "source": [
        "# Loding the training dataset. Refer to write up section 2 to understand the structure\n",
        "dataset     = np.load('dataset/wiki.train.npy', allow_pickle=True)\n",
        "\n",
        "# The dataset does not have <sos> and <eos> because they are just regular articles. \n",
        "# TODO: Add <sos> and <eos> to every article in the dataset.\n",
        "# Before doing do, try printing the dataset to see if they are words or integers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cell_id": "09f3a2efaeef49ef9f4c0b2b9a614cca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution": {
          "iopub.execute_input": "2022-08-10T14:02:12.888156Z",
          "iopub.status.busy": "2022-08-10T14:02:12.884281Z",
          "iopub.status.idle": "2022-08-10T14:02:12.960590Z",
          "shell.execute_reply": "2022-08-10T14:02:12.958805Z",
          "shell.execute_reply.started": "2022-08-10T14:02:12.888058Z"
        },
        "execution_millis": 46,
        "execution_start": 1679856368507,
        "id": "x5znxQhLSwRC",
        "outputId": "994601b9-86c5-4406-b37f-be48b6d3244a",
        "source_hash": "42e4c03c",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation shapes    :  (128, 21) (128,)\n",
            "Test shapes          :  (128, 21)\n",
            "x:  <sos> Division of the V. Corps ) , nine battalions , three squadrons , three guns . <eol> 3rd Division under\n",
            "y:  command\n"
          ]
        }
      ],
      "source": [
        "# Loading the fixtures for validation and test - prediction\n",
        "fixtures_pred       = np.load('fixtures/prediction.npz')        # validation\n",
        "fixtures_pred_test  = np.load('fixtures/prediction_test.npz')   # test\n",
        "\n",
        "print(\"Validation shapes    : \", fixtures_pred['inp'].shape, fixtures_pred['out'].shape)\n",
        "print(\"Test shapes          : \", fixtures_pred_test['inp'].shape)\n",
        "\n",
        "ind = 96\n",
        "print(\"x: \", ' '.join(VOCAB[fixtures_pred['inp'][ind]]))\n",
        "print(\"y: \", VOCAB[fixtures_pred['out'][ind]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pes7mCr5WdAw",
        "outputId": "57199f71-170e-439f-802f-0da5f474c617"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Gen Shapes          : (128, 31)\n"
          ]
        }
      ],
      "source": [
        "# Loading the test fixtures for generation\n",
        "fixtures_gen_test   = np.load('fixtures/generation_test.npy')   # test\n",
        "\n",
        "print(\"Test Gen Shapes          :\", fixtures_gen_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jO_Qt7O6rL8L"
      },
      "outputs": [],
      "source": [
        "# Example Prediction Dev Input and Output\n",
        "# Optional TODO: You can try printing a few samples from the validation set which has both inputs and outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "aec0165a3f1245dfa52a0cb80dba2578",
        "deepnote_cell_type": "markdown",
        "id": "dHjYhXAOzkrP"
      },
      "source": [
        "# Custom DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "cell_id": "b2e63a7f6dec4a3f98588725a72a8ff2",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution": {
          "iopub.execute_input": "2022-08-10T14:02:13.079390Z",
          "iopub.status.busy": "2022-08-10T14:02:13.078847Z",
          "iopub.status.idle": "2022-08-10T14:02:13.196189Z",
          "shell.execute_reply": "2022-08-10T14:02:13.192167Z",
          "shell.execute_reply.started": "2022-08-10T14:02:13.079324Z"
        },
        "execution_millis": 48,
        "execution_start": 1679856368575,
        "id": "OZNrJ8XvSwRF",
        "source_hash": "a81eaa14",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# class DataLoaderForLanguageModeling(torch.utils.data.DataLoader): # Inherit from torch.utils.data.DataLoader\n",
        "#     \"\"\"\n",
        "#         TODO: Define data loader logic here\n",
        "#     \"\"\"\n",
        "#     # TODO: You can probably add more parameters as well. Eg. sequence length\n",
        "#     def __init__(self, dataset, batch_size, shuffle= True, drop_last= False): \n",
        "        \n",
        "#         # If you remember, these are the standard things which you give while defining a dataloader.\n",
        "#         # Now you are just customizing your dataloader\n",
        "#         self.dataset    = dataset\n",
        "#         self.batch_size = batch_size\n",
        "#         self.shuffle    = shuffle\n",
        "#         self.drop_last  = drop_last\n",
        "\n",
        "#     def __len__(self):\n",
        "#         # What output do you get when you print len(loader)? You get the number of batches\n",
        "#         # Your dataset has (579, ) articles and each article has a specified amount of words.\n",
        "#         # You concatenate the dataset and then batch parts of it according to the sequence length\n",
        "#         # TODO: return the number of batches\n",
        "#         # If you are using variable sequence_length, the length might not be fixed \n",
        "#         return NotImplemented\n",
        "\n",
        "#     def __iter__(self):\n",
        "#         # TODOs: \n",
        "#         # 1. Shuffle data if shuffle is True\n",
        "#         # 2. Concatenate articles and drop extra words\n",
        "#         # 3. Divide the concetenated dataset into inputs and targets. How do they vary? \n",
        "#         # 4. Reshape the inputs and targets into batches (think about the final shape)\n",
        "#         # 5. Loop though the batches and yield the input and target according to the sequence length\n",
        "\n",
        "#         if self.shuffle:\n",
        "#             # TODO\n",
        "#             NotImplemented\n",
        "\n",
        "#         num_batches = NotImplemented\n",
        "        \n",
        "#         batch_idx = 0\n",
        "#         if self.drop_last:\n",
        "#             # TODO\n",
        "#             NotImplemented\n",
        "\n",
        "#         while batch_idx < num_batches:\n",
        "#             yield NotImplemented\n",
        "\n",
        "class ShiftLabelDataSet(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, seq_len): \n",
        "        super().__init__()\n",
        "        self.data = np.concatenate(data)\n",
        "        self.seq_len = seq_len\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        # x, y =  self.data[index:index+self.seq_len], self.data[index+1:index+self.seq_len+1]\n",
        "        # return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "        l, r = index * self.seq_len, (index + 1) * self.seq_len\n",
        "        x, y = self.data[l:r], self.data[l+1:r+1]\n",
        "        x, y = torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n",
        "        return x, y\n",
        "    \n",
        "    def __len__(self):\n",
        "        return (len(self.data) - 1) // self.seq_len\n",
        "\n",
        "class DataLoaderForLanguageModeling(torch.utils.data.DataLoader): \n",
        "    def __init__(self, dataset, seq_len=2, drop_last=False, **kwargs): \n",
        "        dataset = ShiftLabelDataSet(dataset, seq_len)\n",
        "        super().__init__(\n",
        "            dataset=dataset, \n",
        "            **kwargs\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "cell_id": "773573c8374048d4bcb5a67b905ee2e0",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 3,
        "execution_start": 1679856368714,
        "id": "fBZSzmy10M9M",
        "source_hash": "27952b8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 1]) torch.Size([32, 1]) torch.int64 torch.int64\n",
            "x:  I\n",
            "y:  was\n"
          ]
        }
      ],
      "source": [
        "# Some sanity checks\n",
        "\n",
        "dl = DataLoaderForLanguageModeling(\n",
        "    dataset     = dataset, \n",
        "    batch_size  = 32, \n",
        "    shuffle     = True, \n",
        "    drop_last   = True,\n",
        "    seq_len     = 1,\n",
        "    pin_memory  = False,\n",
        ")\n",
        "\n",
        "inputs, targets = next(iter(dl))\n",
        "print(inputs.shape, targets.shape, inputs.dtype, targets.dtype)\n",
        "\n",
        "for x, y in dl:\n",
        "    print(\"x: \", ' '.join([VOCAB[i] for i in x[0, :]]))\n",
        "    print(\"y: \", ' '.join([VOCAB[i] for i in y[0, :]]))\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "0e75c3c3318d481aa99230d81eb68c13",
        "deepnote_cell_type": "markdown",
        "id": "WcWU0YlnzmVM"
      },
      "source": [
        "# LanguageModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "cebwoorWttWe"
      },
      "outputs": [],
      "source": [
        "# Here comes the main portion of this HW.\n",
        "# You can do this with a regular LSTM similar to HW3P2. \n",
        "# However, using LSTMCells will make this Language model very similar to the decoder in HW4P2 and we recommend you use that for writing resuable code.\n",
        "\n",
        "class LockedDropout(torch.nn.Module):\n",
        "    def __init__(self, dropout, axis=0): \n",
        "        super().__init__()\n",
        "        self.dropout = dropout\n",
        "    \n",
        "    def forward(self, x):\n",
        "        if not self.training or not self.dropout:\n",
        "            return x\n",
        "        mask_size = x.shape\n",
        "        mask = x.new_empty(mask_size, requires_grad=False).bernoulli_(1 - self.dropout)\n",
        "        mask = mask / (1 - self.dropout)\n",
        "        mask = mask.expand_as(x)\n",
        "        return mask * x\n",
        "\n",
        "\n",
        "class LanguageModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_size): # TODO: Add more parameters if you want\n",
        "        super().__init__()\n",
        "\n",
        "        # For all the layers which you will define, please read the documentation thoroughly before implementation\n",
        "\n",
        "        self.token_embedding    = torch.nn.Embedding( vocab_size, embed_dim )\n",
        "        self.input_dropout      = torch.nn.Dropout(0.4)\n",
        "\n",
        "        self.lstm_cells         = torch.nn.Sequential(\n",
        "            torch.nn.LSTMCell(embed_dim, hidden_size),\n",
        "            torch.nn.LSTMCell(hidden_size, hidden_size),\n",
        "            torch.nn.LSTMCell(hidden_size, hidden_size),\n",
        "        )\n",
        "        self.dropouts = [\n",
        "            LockedDropout(0.3),\n",
        "            LockedDropout(0.4),\n",
        "            LockedDropout(0.1),\n",
        "        ]\n",
        "\n",
        "        self.linear = torch.nn.Linear(hidden_size, embed_dim)\n",
        "        self.final_linear = torch.nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "        self.final_linear.weight = self.token_embedding.weight\n",
        "        \n",
        "        self.token_probability = torch.nn.Sequential(\n",
        "            self.linear,\n",
        "            self.final_linear,\n",
        "        )\n",
        "                \n",
        "        # So the basic pipline is:\n",
        "        # word -> embedding -> lstm -> projection (linear) to get  probability distribution\n",
        "        # And this is happening across all time steps\n",
        "\n",
        "    def rnn_step(self, embedding, hidden_states_list):\n",
        "        x = embedding # dim = (batch, embed_dim)\n",
        "\n",
        "        for i in range(len(self.lstm_cells)):\n",
        "            hc = hidden_states_list[i]\n",
        "            h, c = self.lstm_cells[i](x, hc)\n",
        "            hidden_states_list[i] = (h, c)\n",
        "            x = h\n",
        "            x = self.dropouts[i](x)\n",
        "                       \n",
        "        return x, hidden_states_list\n",
        "\n",
        "    def predict(self, x):\n",
        "        # Refer to Section 1.3.1 to understand this function\n",
        "        if not torch.is_tensor(x):\n",
        "            x = torch.tensor(x).long().to(DEVICE)\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            y_probs, _ = self.forward(x) # dim = (batch, seq_len, vocab_size)\n",
        "            y_prob = y_probs[:, -1, :] # dim = (batch, vocab_size)\n",
        "            \n",
        "        return y_prob\n",
        "\n",
        "    def generate(self, x, timesteps): \n",
        "        # Refer to section 1.3.2 to understand this function\n",
        "        # Important Note: We do not draw <eos> from the distribution unlike the writeup\n",
        "        if not torch.is_tensor(x):\n",
        "            x = torch.tensor(x).long().to(DEVICE)\n",
        "\n",
        "        # TODO: Pass the input sequence through the model \n",
        "        # Obtain the probability distribution and hidden_states_list of the last timestep\n",
        "        \n",
        "        token_prob_dist, hidden_states_list     = self.forward(x) # dim = (batch, seq_len, vocab_size), (num_layers, batch, hidden_size)\n",
        "        next_token                              = token_prob_dist[:, -1, :-1].argmax(axis=-1, keepdims=True) # dim = (batch, 1)\n",
        "\n",
        "        generated_sequence  = [next_token] \n",
        "        with torch.inference_mode():\n",
        "            for t in range(timesteps - 1): # Loop through the timesteps\n",
        "                #   TODO: Pass the next_token and hidden_states_list through the model\n",
        "                #   TODO: You will get 2 outputs. What is the shape of the probability distribution?\n",
        "                #   TODO: Get the most probable token for the next timestep\n",
        "\n",
        "                token_prob_dist, hidden_states_list = self.forward(next_token) \n",
        "                # token_prob_dist: dim = (batch, 1, vocab_size) \n",
        "                next_token = token_prob_dist[:, -1, :-1].argmax(axis=-1, keepdims=True) # dim = (batch, 1)\n",
        "                generated_sequence.append(next_token)\n",
        "            \n",
        "        generated_sequence = torch.stack(generated_sequence, dim= 1) # dim = (batch, timesteps)\n",
        "        return generated_sequence.squeeze()\n",
        "\n",
        "    # We are also having a hidden_states_list parameter because you need that in generation\n",
        "    def forward(self, x, hidden_states_list= None): # train model\n",
        "        # x (Batch, Seq_len)\n",
        "        # Note: you dont have to return the sum of log probabilities according to Pseudocode 1 in the writeup\n",
        "        # However, feel free to calculate and print it if you are curious\n",
        "\n",
        "        batch_size, timesteps   = x.shape \n",
        "\n",
        "        token_prob_distribution = [] # list which will contain probability distributions for all timesteps\n",
        "        # Initializing the hidden states\n",
        "        hidden_states_list      = [None]*len(self.lstm_cells) if hidden_states_list == None else hidden_states_list       \n",
        "\n",
        "        # input: (batch, seq_len)\n",
        "        # x = self.input_dropout(x)\n",
        "        token_embeddings        = self.token_embedding(x)\n",
        "        # output:  (batch, seq_len, embed_dim)\n",
        "\n",
        "        for t in range(timesteps): # LSTMCell is for just 1 timestep. Hence you need to loop through the total timesteps\n",
        "\n",
        "            token_embedding_t           = token_embeddings[:, t, :]\n",
        "\n",
        "            rnn_out, hidden_states_list = self.rnn_step(token_embedding_t, hidden_states_list)\n",
        "            # rnn_out: (batch, hidden_size)\n",
        "            \n",
        "            token_prob_dist_t           = self.token_probability(rnn_out) \n",
        "            # output: (batch, vocab_size)\n",
        "\n",
        "            token_prob_distribution.append(token_prob_dist_t) \n",
        "\n",
        "        token_prob_distribution = torch.stack(token_prob_distribution, dim= 1)\n",
        "        # output: (batch, seq_len, vocab_size)\n",
        "\n",
        "        return token_prob_distribution, hidden_states_list "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "8ed5a6ef54f9446fab752b79c70a0216",
        "deepnote_cell_type": "markdown",
        "id": "TlWF_bpLznup"
      },
      "source": [
        "# Trainer Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "cell_id": "8ea986fc372643389d1ab4c445659e9d",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": true,
        "execution": {
          "iopub.execute_input": "2022-08-10T14:02:13.440820Z",
          "iopub.status.busy": "2022-08-10T14:02:13.440281Z",
          "iopub.status.idle": "2022-08-10T14:02:13.644455Z",
          "shell.execute_reply": "2022-08-10T14:02:13.642614Z",
          "shell.execute_reply.started": "2022-08-10T14:02:13.440752Z"
        },
        "id": "kIvZOIfjSwRK",
        "source_hash": "451a140f",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Unlike all the P2s, we are using a Trainer class for this HW.\n",
        "# Many researchers also use classes like this for training. You may have encountered them in your project as well.\n",
        "# You dont have to complete everything in this class, you only need to complete the train function.\n",
        "# However, its good to go through the code and see what it does. \n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model, loader, optimizer, criterion, scheduler, max_epochs= 1, run_id= 'exp'):\n",
        "        \"\"\"\n",
        "            Use this class to train your model\n",
        "        \"\"\"\n",
        "        # feel free to add any other parameters here\n",
        "        self.model      = model\n",
        "        self.loader     = loader\n",
        "        self.optimizer  = optimizer\n",
        "        self.criterion  = criterion\n",
        "        self.scheduler = scheduler\n",
        "\n",
        "        self.train_losses           = []\n",
        "        self.val_losses             = []\n",
        "        self.prediction_probs       = []\n",
        "        self.prediction_probs_test  = []\n",
        "        self.generated_texts_test   = []\n",
        "        self.epochs                 = 0\n",
        "        self.max_epochs             = max_epochs\n",
        "        self.run_id                 = run_id\n",
        "\n",
        "\n",
        "    def calculate_loss(self, out, target):\n",
        "        # output: (B, T, Vocab_size) - probability distributions\n",
        "        # target: (B, T)\n",
        "        # Read the documentation of CrossEntropyLoss and try to understand how it takes inputs\n",
        "\n",
        "        # Tip: If your target is of shape (B, T) it means that you have B batches with T words. \n",
        "        # Tip: What is the total number of words in this batch? \n",
        "        # Tip: Crossentropy calculates the loss between a label and its probability distribution.\n",
        "\n",
        "        out     = out.reshape(-1, out.shape[-1]) # dim = (B*T, Vocab_size)\n",
        "        targets = target.reshape(-1) # dim = (B*T, )\n",
        "        loss    = self.criterion(out, targets)\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "        self.model.train() # set to training mode\n",
        "        self.model.to(DEVICE)\n",
        "        epoch_loss  = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        scaler = torch.cuda.amp.GradScaler()\n",
        "        bar = tqdm(total=len(self.loader), desc='Training', position=0, leave=True)\n",
        "\n",
        "        for batch_num, (inputs, targets) in enumerate(self.loader):\n",
        "\n",
        "            # TODO: Complete the loop. You should be able to complete this without any helper comments after 3 HWs\n",
        "            # Tip: Mixed precision training\n",
        "            # For loss calculation, use the calculate_loss function. You need to complete it before using.\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            with torch.cuda.amp.autocast():\n",
        "                inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "                out, _ = self.model(inputs)\n",
        "                loss = self.calculate_loss(out, targets)\n",
        "                \n",
        "            # loss.backward()\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(self.optimizer)\n",
        "            scaler.update()\n",
        " \n",
        "            loss = loss.item()\n",
        "            epoch_loss += loss\n",
        "\n",
        "            del inputs, targets, out, loss\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            bar.set_postfix(\n",
        "                loss = epoch_loss / (batch_num + 1),\n",
        "                lr = self.optimizer.param_groups[0]['lr']\n",
        "            )\n",
        "            bar.update()\n",
        "        \n",
        "        \n",
        "        epoch_loss = epoch_loss / (batch_num + 1)\n",
        "        self.epochs += 1\n",
        "        print('[TRAIN] \\tEpoch [%d/%d] \\tLoss: %.4f \\tLr: %.6f'\n",
        "                      % (self.epochs, self.max_epochs, epoch_loss, self.optimizer.param_groups[0]['lr']))\n",
        "        self.train_losses.append(epoch_loss)\n",
        "\n",
        "\n",
        "    \n",
        "    def test(self): # Don't change this function\n",
        "        \n",
        "        self.model.eval() # set to eval mode\n",
        "        prediction_probs     = self.model.predict(fixtures_pred['inp']).detach().cpu().numpy() # get predictions\n",
        "        self.prediction_probs.append(prediction_probs)\n",
        "\n",
        "        generated_indexes_test   = self.model.generate(fixtures_gen_test, 10).detach().cpu().numpy() # generated predictions for 10 words\n",
        "\n",
        "        nll                   = get_prediction_nll(prediction_probs, fixtures_pred['out'])\n",
        "        generated_texts_test  = make_generation_text(fixtures_gen_test, generated_indexes_test, VOCAB)\n",
        "        self.val_losses.append(nll)\n",
        "        \n",
        "        self.generated_texts_test.append(generated_texts_test)\n",
        "        \n",
        "        # generate predictions for test data\n",
        "        prediction_probs_test = self.model.predict(fixtures_pred_test['inp']).detach().cpu().numpy() # get predictions\n",
        "        self.prediction_probs_test.append(prediction_probs_test)\n",
        "            \n",
        "        print('[VAL] \\tEpoch [%d/%d] \\tLoss: %.4f'\n",
        "                      % (self.epochs, self.max_epochs, nll))\n",
        "        return nll\n",
        "\n",
        "    \n",
        "    def save(self): # Don't change this function\n",
        "\n",
        "        model_path = os.path.join('hw4/experiments', self.run_id, 'model-{}.pkl'.format(self.epochs))\n",
        "        torch.save({'state_dict': self.model.state_dict()}, model_path)\n",
        "        np.save(os.path.join('hw4/experiments', self.run_id, 'prediction-probs-{}.npy'.format(self.epochs)), self.prediction_probs[-1])\n",
        "        np.save(os.path.join('hw4/experiments', self.run_id, 'prediction-probs-test-{}.npy'.format(self.epochs)), self.prediction_probs_test[-1])\n",
        "\n",
        "        with open(os.path.join('hw4/experiments', self.run_id, 'generated-texts-{}-test.txt'.format(self.epochs)), 'w') as fw:\n",
        "            fw.write(self.generated_texts_test[-1])\n",
        "    \n",
        "    def load(self, run_id, epochs): \n",
        "        path = os.path.join('hw4/experiments', str(run_id), 'model-{}.pkl'.format(epochs))\n",
        "        self.model.load_state_dict(torch.load(path)['state_dict'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "db5de3ac0c6e48ca9cff0cd79bef7ae8",
        "deepnote_cell_type": "markdown",
        "id": "E6NKG0j8zsv-"
      },
      "source": [
        "# Experiment setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "cell_id": "7fc44ee4771a42f996d0a00d35529fb6",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": true,
        "execution": {
          "iopub.execute_input": "2022-08-10T14:02:13.852171Z",
          "iopub.status.busy": "2022-08-10T14:02:13.850633Z",
          "iopub.status.idle": "2022-08-10T14:02:13.927227Z",
          "shell.execute_reply": "2022-08-10T14:02:13.924500Z",
          "shell.execute_reply.started": "2022-08-10T14:02:13.852093Z"
        },
        "id": "TiUrjbEjSwRQ",
        "source_hash": "f7524436",
        "tags": []
      },
      "outputs": [],
      "source": [
        "\n",
        "configs = dict(\n",
        "    batch_size  = 256,\n",
        "    num_epochs  = 20, # 10 or 20 epochs should be enough given the model is good\n",
        "\n",
        "    init_lr     = 1e-3,\n",
        "    seq_len     = 20,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "cell_id": "4aaccf1c32fa480a9a15e8bb8bc4d9e4",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": true,
        "execution": {
          "iopub.execute_input": "2022-08-10T14:02:14.110787Z",
          "iopub.status.busy": "2022-08-10T14:02:14.109778Z",
          "iopub.status.idle": "2022-08-10T14:02:14.929087Z",
          "shell.execute_reply": "2022-08-10T14:02:14.925078Z",
          "shell.execute_reply.started": "2022-08-10T14:02:14.110707Z"
        },
        "id": "DbHH6zXTSwRa",
        "source_hash": "2acff566",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "LanguageModel                            [32, 1, 33280]            --\n",
              "├─Embedding: 1-1                         [32, 1, 256]              8,519,680\n",
              "├─Sequential: 1-2                        --                        --\n",
              "│    └─LSTMCell: 2-1                     [32, 512]                 1,576,960\n",
              "│    └─LSTMCell: 2-2                     [32, 512]                 2,101,248\n",
              "│    └─LSTMCell: 2-3                     [32, 512]                 2,101,248\n",
              "├─Sequential: 1-3                        [32, 33280]               --\n",
              "│    └─Linear: 2-4                       [32, 256]                 131,328\n",
              "│    └─Linear: 2-5                       [32, 33280]               8,552,960\n",
              "==========================================================================================\n",
              "Total params: 22,983,424\n",
              "Trainable params: 22,983,424\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.GIGABYTES): 95.24\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 9.04\n",
              "Params size (MB): 91.93\n",
              "Estimated Total Size (MB): 100.98\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model       = LanguageModel(len(VOCAB), 256, 512)\n",
        "\n",
        "loader      = DataLoaderForLanguageModeling(\n",
        "                                dataset    = dataset,   \n",
        "                                seq_len    = configs['seq_len'],\n",
        "                                shuffle    = True,\n",
        "                                batch_size = configs['batch_size'],\n",
        "                                drop_last  = True,\n",
        "                                pin_memory = True,\n",
        ")\n",
        "\n",
        "criterion   = torch.nn.CrossEntropyLoss() \n",
        "\n",
        "optimizer   = torch.optim.Adam(model.parameters(), lr= configs['init_lr'])\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1, verbose=True)\n",
        "\n",
        "from torchinfo import summary\n",
        "summary(model, input_data=inputs)    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "cell_id": "aaff53cf948e44b7b9bd49cbcad0ac58",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": true,
        "execution": {
          "iopub.execute_input": "2022-08-10T14:02:13.931258Z",
          "iopub.status.busy": "2022-08-10T14:02:13.930204Z",
          "iopub.status.idle": "2022-08-10T14:02:14.107883Z",
          "shell.execute_reply": "2022-08-10T14:02:14.105987Z",
          "shell.execute_reply.started": "2022-08-10T14:02:13.931185Z"
        },
        "id": "2HCVG5YISwRW",
        "source_hash": "c9f4594a",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving models, prediction prbabilities, and generated texts to ./hw4/experiments/1699830189\n"
          ]
        }
      ],
      "source": [
        "# Dont change this cell\n",
        "\n",
        "run_id = str(int(time.time()))\n",
        "if not os.path.exists('./hw4/experiments'):\n",
        "    os.mkdir('./hw4/experiments')\n",
        "os.mkdir('./hw4/experiments/%s' % run_id)\n",
        "print(\"Saving models, prediction prbabilities, and generated texts to ./hw4/experiments/%s\" % run_id)\n",
        "\n",
        "# The object of the Trainer class takes in everything\n",
        "trainer = Trainer(\n",
        "    model       = model, \n",
        "    loader      = loader, \n",
        "\n",
        "    optimizer   = optimizer,\n",
        "    criterion   = criterion,\n",
        "    scheduler   = scheduler,\n",
        "    \n",
        "    max_epochs  = configs['num_epochs'], \n",
        "    run_id      = run_id\n",
        ")\n",
        "\n",
        "# trainer.load(1699816606, 10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "V0dy4CpXJgN2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   0%|          | 0/406 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 406/406 [00:43<00:00,  9.31it/s, loss=6.99, lr=0.001]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN] \tEpoch [1/20] \tLoss: 6.9894 \tLr: 0.001000\n",
            "[VAL] \tEpoch [1/20] \tLoss: 5.6826\n",
            "Saving model, prediction probabilities and generated texts for epoch 1 with NLL: 5.682583\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 406/406 [00:44<00:00,  9.15it/s, loss=6.16, lr=0.001]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN] \tEpoch [2/20] \tLoss: 6.1610 \tLr: 0.001000\n",
            "[VAL] \tEpoch [2/20] \tLoss: 5.3639\n",
            "Saving model, prediction probabilities and generated texts for epoch 2 with NLL: 5.363891\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 406/406 [00:44<00:00,  9.11it/s, loss=5.88, lr=0.001]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN] \tEpoch [3/20] \tLoss: 5.8826 \tLr: 0.001000\n",
            "[VAL] \tEpoch [3/20] \tLoss: 5.0936\n",
            "Saving model, prediction probabilities and generated texts for epoch 3 with NLL: 5.0936007\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 406/406 [00:44<00:00,  9.03it/s, loss=5.67, lr=0.001]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN] \tEpoch [4/20] \tLoss: 5.6749 \tLr: 0.001000\n",
            "[VAL] \tEpoch [4/20] \tLoss: 4.9913\n",
            "Saving model, prediction probabilities and generated texts for epoch 4 with NLL: 4.9912815\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 406/406 [00:45<00:00,  8.94it/s, loss=5.51, lr=0.001]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN] \tEpoch [5/20] \tLoss: 5.5101 \tLr: 0.001000\n",
            "[VAL] \tEpoch [5/20] \tLoss: 4.8903\n",
            "Saving model, prediction probabilities and generated texts for epoch 5 with NLL: 4.8903437\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 406/406 [00:45<00:00,  8.92it/s, loss=5.37, lr=0.001]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN] \tEpoch [6/20] \tLoss: 5.3748 \tLr: 0.001000\n",
            "[VAL] \tEpoch [6/20] \tLoss: 4.8487\n",
            "Saving model, prediction probabilities and generated texts for epoch 6 with NLL: 4.848731\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 406/406 [00:46<00:00,  8.69it/s, loss=5.26, lr=0.001]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN] \tEpoch [7/20] \tLoss: 5.2609 \tLr: 0.001000\n",
            "[VAL] \tEpoch [7/20] \tLoss: 4.8964\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 406/406 [00:47<00:00,  8.55it/s, loss=5.16, lr=0.001]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN] \tEpoch [8/20] \tLoss: 5.1633 \tLr: 0.001000\n",
            "[VAL] \tEpoch [8/20] \tLoss: 4.7567\n",
            "Saving model, prediction probabilities and generated texts for epoch 8 with NLL: 4.7567296\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 406/406 [00:47<00:00,  8.52it/s, loss=5.08, lr=0.001]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN] \tEpoch [9/20] \tLoss: 5.0784 \tLr: 0.001000\n",
            "[VAL] \tEpoch [9/20] \tLoss: 4.7158\n",
            "Saving model, prediction probabilities and generated texts for epoch 9 with NLL: 4.715826\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 406/406 [00:47<00:00,  8.49it/s, loss=5, lr=0.001]   \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN] \tEpoch [10/20] \tLoss: 5.0022 \tLr: 0.001000\n",
            "[VAL] \tEpoch [10/20] \tLoss: 4.7457\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 406/406 [00:47<00:00,  8.46it/s, loss=4.93, lr=0.001]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN] \tEpoch [11/20] \tLoss: 4.9345 \tLr: 0.001000\n",
            "[VAL] \tEpoch [11/20] \tLoss: 4.7585\n",
            "Epoch 00011: reducing learning rate of group 0 to 5.0000e-04.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 406/406 [00:48<00:00,  8.45it/s, loss=4.82, lr=0.0005]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN] \tEpoch [12/20] \tLoss: 4.8208 \tLr: 0.000500\n",
            "[VAL] \tEpoch [12/20] \tLoss: 4.8560\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 406/406 [00:48<00:00,  8.44it/s, loss=4.78, lr=0.0005]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN] \tEpoch [13/20] \tLoss: 4.7760 \tLr: 0.000500\n",
            "[VAL] \tEpoch [13/20] \tLoss: 4.8433\n",
            "Epoch 00013: reducing learning rate of group 0 to 2.5000e-04.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 406/406 [00:48<00:00,  8.43it/s, loss=4.71, lr=0.00025]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN] \tEpoch [14/20] \tLoss: 4.7119 \tLr: 0.000250\n",
            "[VAL] \tEpoch [14/20] \tLoss: 4.8636\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 406/406 [00:48<00:00,  8.41it/s, loss=4.69, lr=0.00025]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN] \tEpoch [15/20] \tLoss: 4.6872 \tLr: 0.000250\n",
            "[VAL] \tEpoch [15/20] \tLoss: 4.8477\n",
            "Epoch 00015: reducing learning rate of group 0 to 1.2500e-04.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 406/406 [00:48<00:00,  8.38it/s, loss=4.65, lr=0.000125]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN] \tEpoch [16/20] \tLoss: 4.6514 \tLr: 0.000125\n",
            "[VAL] \tEpoch [16/20] \tLoss: 4.8075\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 406/406 [00:49<00:00,  8.28it/s, loss=4.64, lr=0.000125]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN] \tEpoch [17/20] \tLoss: 4.6399 \tLr: 0.000125\n",
            "[VAL] \tEpoch [17/20] \tLoss: 4.7099\n",
            "Saving model, prediction probabilities and generated texts for epoch 17 with NLL: 4.709949\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 406/406 [00:48<00:00,  8.37it/s, loss=4.63, lr=0.000125]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN] \tEpoch [18/20] \tLoss: 4.6286 \tLr: 0.000125\n",
            "[VAL] \tEpoch [18/20] \tLoss: 4.7582\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 406/406 [00:48<00:00,  8.37it/s, loss=4.62, lr=0.000125]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN] \tEpoch [19/20] \tLoss: 4.6187 \tLr: 0.000125\n",
            "[VAL] \tEpoch [19/20] \tLoss: 4.8409\n",
            "Epoch 00019: reducing learning rate of group 0 to 6.2500e-05.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 406/406 [00:48<00:00,  8.38it/s, loss=4.6, lr=6.25e-5] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN] \tEpoch [20/20] \tLoss: 4.6014 \tLr: 0.000063\n",
            "[VAL] \tEpoch [20/20] \tLoss: 4.8329\n"
          ]
        }
      ],
      "source": [
        "\n",
        "model.to(DEVICE)\n",
        "# # %%time\n",
        "best_nll = 1e30 \n",
        "for epoch in range(configs['num_epochs']):\n",
        "    trainer.train()\n",
        "    nll = trainer.test()\n",
        "    scheduler.step(nll)\n",
        "    if nll < best_nll:\n",
        "        best_nll = nll\n",
        "        print(\"Saving model, prediction probabilities and generated texts for epoch \"+str(epoch+1)+\" with NLL: \"+ str(best_nll))\n",
        "        trainer.save()    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'1699830189'"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "run_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "i_gYqXq9Jgo1"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure()\n",
        "plt.plot(range(1, trainer.epochs + 1), trainer.train_losses, label='Training losses')\n",
        "plt.plot(range(1, trainer.epochs + 1), trainer.val_losses, label='Validation losses')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('NLL')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluating generations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 128/128 [00:22<00:00,  5.57it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your mean perplexity for generated sequences: 782.4295242731397\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Now that you have trained your model and got satisfactory validation NLL on the single token prediction task,\n",
        "# you can evaluate the generations you created too. We will use the perplexity metric to evaluate generations\n",
        "# using a large language model available through the OpenAI API. Read the handout for instructions on how to sign\n",
        "# up for the API and obtain and API key. Once you add credits to your account, run this cell to get the perplexity.\n",
        "# You will submit this perplexity value for grading the generation component of this homework.\n",
        "# A perplexity of under <tbd> will give you full credit on the generation part.\n",
        "\n",
        "# DO NOT CHANGE THE CODE IN THIS CELL EXCEPT submission_run_id, submission_epoch, AND api_key \n",
        "# PLEASE BE HONEST IN REPORTING THE PERPLEXITY VALUE!\n",
        "# WE WILL RANDOMLY CHECK SOME SUBMISSIONS USING THE SAME CODE AS THIS AND A BIG DIFFERENCE IN PERPLEXITY WILL RESULT IN AN AIV.\n",
        "\n",
        "import openai\n",
        "\n",
        "# Fill the run id and epoch number to be used for submission.\n",
        "# You will use the same run id and epoch number to generate the handin.\n",
        "submission_run_id = run_id\n",
        "submission_epoch = 17\n",
        "\n",
        "n_tests = 128\n",
        "\n",
        "with open(os.path.join('hw4/experiments', submission_run_id, 'generated-texts-{}-test.txt'.format(submission_epoch)), 'r', encoding='utf-8') as f:\n",
        "    generated = list(f)\n",
        "\n",
        "assert len(generated) == n_tests\n",
        "for item in generated:\n",
        "    assert type(item) is str\n",
        "\n",
        "parsed_generated = []\n",
        "\n",
        "for text in generated:\n",
        "    start_index = text.index(\"<sos>\")\n",
        "    temp = text[start_index+6:]\n",
        "    generation_start_index = temp.index(\"| \")\n",
        "    parsed_text = temp[:generation_start_index] + temp[generation_start_index+2:]\n",
        "    parsed_text = parsed_text.replace(\"<eol>\", \"\\n\")\n",
        "    parsed_generated.append(parsed_text)\n",
        "\n",
        "def perplexity(text, modelname):\n",
        "    \"\"\"Compute the perplexity of the provided text.\"\"\"\n",
        "    completion = openai.Completion.create(\n",
        "        model=modelname,\n",
        "        prompt=text,\n",
        "        logprobs=0,\n",
        "        max_tokens=0,\n",
        "        temperature=1.0,\n",
        "        echo=True)\n",
        "    token_logprobs = completion['choices'][0]['logprobs']['token_logprobs']\n",
        "    ll = np.mean([i for i in token_logprobs if i is not None])\n",
        "    ppl = np.exp(-ll)\n",
        "    return ppl\n",
        "\n",
        "# Add you API key here to get perplexity. However, delete the key from the notebook before creating the handin.\n",
        "# REMEMBER: ALWAYS KEEP YOUR API KEYS AND SECRETS SECURE.\n",
        "openai.api_key = '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' # TODO\n",
        "modelname = 'text-embedding-ada-002'\n",
        "\n",
        "perps = [perplexity(text, modelname) for text in tqdm(parsed_generated)]\n",
        "avg_perp = np.mean(perps)\n",
        "\n",
        "# Report this number when running the makefile to create the handin\n",
        "print(\"Your mean perplexity for generated sequences: {}\".format(avg_perp))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBqjqy-EyU27"
      },
      "source": [
        "# Create handin\n",
        "Navigate to the handout directory to run the below cell. This command will create the handin with all the required files (including attention.py). So make sure you have the entire handout directory wherever you are running this notebook (local machine, Colab, AWS, etc.). This command requires that this completed notebook be in the hw4 folder inside the handout directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "hWRyPvWmgLQs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "echo 782.4295242731397 > generation_ppl.txt\n",
            "cp hw4/experiments/1699830189/prediction-probs-test-17.npy prediction_probs.npy\n",
            "cp hw4/experiments/1699830189/generated-texts-17-test.txt generated_texts.txt\n",
            "cp hw4/hw4p1.ipynb training.ipynb\n",
            "cp hw4/attention.py attention.py\n",
            "tar -cvzf handin.tar training.ipynb prediction_probs.npy generated_texts.txt generation_ppl.txt attention.py\n",
            "training.ipynb\n",
            "prediction_probs.npy\n",
            "generated_texts.txt\n",
            "generation_ppl.txt\n",
            "attention.py\n",
            "rm -f generated_texts.txt prediction_probs.npy training.ipynb generation_ppl.txt attention.py\n",
            "1699830189 17 782.4295242731397\n"
          ]
        }
      ],
      "source": [
        "# TODO: Generate the handin to submit to autolab\n",
        "# For example make runid=1234 epoch=4 genppl=123.4\n",
        "run_id = run_id\n",
        "epoch = 17\n",
        "genppl = avg_perp\n",
        "\n",
        "!make runid={run_id} epoch={epoch} ppl={avg_perp}\n",
        "print(run_id, epoch, avg_perp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<unk> , and the <unk> , and the <unk> , and the <unk> , and the <unk> , and the'"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentense = \"If you want to split a string based on multiple places , as\"\n",
        "sentense = [VOCAB.tolist().index(i) for i in sentense.split()]\n",
        "sentense = np.array(sentense).reshape(1, -1)\n",
        "gen = trainer.model.generate(sentense, 20).detach().cpu().numpy()\n",
        "gen = ' '.join(VOCAB[i] if VOCAB[i] != '<eol>' else '\\n' for i in gen)\n",
        "gen\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "EB2bOV3bzYLR",
        "INh9p3v3zbF_",
        "u-R794-0zc9V"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "deepnote": {},
    "deepnote_execution_queue": [],
    "deepnote_notebook_id": "989a3c3836794109ac641230122845a3",
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
